{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atifhsn/Desktop/projects/UC llm 2.0/env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of India is New Delhi.' response_metadata={'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "# function to get llm response\n",
    "def get_openai_response(question: str):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\",\n",
    "                 temperature = 0.6\n",
    "                 )\n",
    "    response = llm.invoke(question)\n",
    "    return response\n",
    "\n",
    "res = get_openai_response(\"what is the capital of India\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1: when people want to share a webpage and ask you to generate question from here\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_article_text(url):\n",
    "    # Fetch the webpage content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the main article body\n",
    "    article_body = soup.find('body')\n",
    "\n",
    "    # Extract text from the body\n",
    "    article_text = article_body.get_text()\n",
    "\n",
    "    return article_text\n",
    "\n",
    "# lets try now\n",
    "url = 'https://www.computerworld.com/article/3697649/what-are-large-language-models-and-how-are-they-used-in-generative-ai.html'\n",
    "# article_text = get_article_text(url)\n",
    "# if article_text:\n",
    "#     main_text = article_text\n",
    "#     print(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/atifhsn/Desktop/projects/UC llm 2.0\n"
     ]
    }
   ],
   "source": [
    "# Use 2. When people want to upload a pdf and ask you to generate question from it\n",
    "import os\n",
    "\n",
    "pdf_directory = os.getcwd()\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "# Opening the pdf\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_file_name = filename\n",
    "        pdf_file = open(os.path.join(pdf_directory, filename), \"rb\")\n",
    "\n",
    "# Reading the pdf\n",
    "pdf_reader = PdfReader(pdf_file)\n",
    "all_text = \"\"\n",
    "# make it limited. min(5, len(pages))\n",
    "for idx, page in enumerate(pdf_reader.pages):\n",
    "    all_text += page.extract_text()\n",
    "    if idx > 4:\n",
    "        break\n",
    "\n",
    "# print(all_text)\n",
    "\n",
    "# Use 3. let the user input an story to generate questions from\n",
    "    \n",
    "# Use 4. let the use share a video link to generate questions from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([all_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Risk Management Examination Manual of Credit Card Activities                                      Chapter II \\n \\n \\n                                   \\nII. CREDIT CARDS – GENERAL OVERVIEW                                                      \\n \\n \\nWHAT IS A CREDIT CARD \\n \\nIn its non-physical form, a credit card repres ents a payment mechanism which facilitates both \\nconsumer and commercial business transactions, including purchases and cash advances .  A \\ncredit card generally operates as a substitute for cash or a check and most often provides an \\nunsecured revolving line of credit.  The borrower is required to pay at least part of the card’s \\noutstanding balance each billing cycle , depending on the terms as set forth in the cardholder \\nagreement .  As the debt reduces, the available credit  increases for accounts in good standing.  \\nThese complex financial arrangements have ever-shifting terms and prices.  A charge card'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining which embeddings to use\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store in vector db\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(f'faiss_{pdf_file_name}_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is present in the document\n",
    "    feel free to say, \"try ansking something else, this information is not available\", don't provide the wrong answer no matter what is present in the question\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(temperature=0.7)\n",
    "    prompt = PromptTemplate(template=prompt_template, \n",
    "                            input_variables=[\"context\", \"Question\"])\n",
    "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt = prompt)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the question, find top k  similar documents\n",
    "user_input = \"What is a credit card?\"\n",
    "\n",
    "loaded_db = FAISS.load_local(f'faiss_{pdf_file_name}_index', embeddings, allow_dangerous_deserialization=True)\n",
    "docs = loaded_db.similarity_search(user_input)\n",
    "\n",
    "chain = get_conversational_chain()\n",
    "# give the answer\n",
    "\n",
    "response = chain({\"input_documents\" : docs, \n",
    "                  \"question\": user_input,\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A credit card represents a payment mechanism that facilitates both consumer and commercial business transactions, including purchases and cash advances. It generally operates as a substitute for cash or a check and most often provides an unsecured revolving line of credit. The borrower is required to pay at least part of the card's outstanding balance each billing cycle, depending on the terms as set forth in the cardholder agreement. As the debt reduces, the available credit increases for accounts in good standing. The physical form of a credit card is traditionally a thin, rectangular plastic card with a series of numbers on the front that represent various items such as the applicable network, bank, and account. The back of the card typically contains a magnetic stripe that electronically stores some of the account's information, as well as a cardholder signature box. Additionally, credit cards are often associated with Visa and MasterCard, which operate sophisticated payment networks and provide services to member banks.\n"
     ]
    }
   ],
   "source": [
    "print(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "def get_topics_from_chunk(context: str):\n",
    "    prompt_template = \"\"\"\n",
    "            I will give a context, and you have to tell me what top 3 topics the text might belong to.\n",
    "            if you unable to find any, you can respond with <no_topic>, but dont output any rubbish topics.\n",
    "            do not write anything other than the topics names. also, give the topics in a comma separted way.\\n\\n\n",
    "            context:\\n{context}\\n\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "    prompt = PromptTemplate(template=prompt_template, \n",
    "                            input_variables=['context'])\n",
    "    response = LLMChain(llm=model, prompt=prompt)\n",
    "    return response(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:21<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "all_topics = []\n",
    "from tqdm import tqdm\n",
    "for t in tqdm(texts):\n",
    "    response = get_topics_from_chunk(t.page_content)\n",
    "    response = response['text'].split(\", \")\n",
    "    all_topics.extend([x.strip() for x in response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "most_common_words = Counter([x.lower() for  x in all_topics]).most_common(5)\n",
    "most_common_words_without_count = [word for word, _ in most_common_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy\n"
     ]
    }
   ],
   "source": [
    "# finding the \n",
    "import random\n",
    "toughness_input = random.choice(['Easy', 'Moderate', \"Tough\"])\n",
    "print(toughness_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banking'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_topic = random.choice(most_common_words_without_count[:3])\n",
    "selected_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains.qa_generation.base import QAGenerationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "def generate_qa_pairs(context, topic):\n",
    "    prompt_template = \"\"\"\n",
    "        Given a context, i want you to generate 2 questions of toughness level: Tough out of these three levels\n",
    "        Easy, Moderate and Tough. The question must belong to topic: {topic}. \n",
    "        Make sure the answer to the question you generate belong to the context provided.\n",
    "        give me the question answer pair in json format.\\n\\n\n",
    "        context:\\n{context}\\n\n",
    "        \"\"\"\n",
    "    model = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5)\n",
    "    parser = JsonOutputParser()\n",
    "    prompt = PromptTemplate(template = prompt_template,\n",
    "                            input_variables=['context', 'topic'],\n",
    "                            partial_variables={'format_instructions': parser.get_format_instructions()})\n",
    "    # chat = RetrievalQA.from_chain_type(llm = llm, \n",
    "    #                                    chain_type=\"stuff\",\n",
    "    #                                    retriever = db.as_retriever(),\n",
    "    #                                    return_source_documents=True,\n",
    "    #                                    chain_type_kwargs={\"prompt\": prompt},\n",
    "    #                                    )\n",
    "    # response = load_qa_chain(llm=model, chain_type=\"stuff\", prompt=prompt)\n",
    "    # return response({\"input_documents\" : context, \n",
    "    #                  \"topic\": topic})\n",
    "\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    return chain.invoke({\"context\": context,\n",
    "                  \"topic\": topic})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_for_questions = loaded_db.similarity_search(selected_topic, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = generate_qa_pairs(docs_for_questions, selected_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What are some of the factors that have forced credit card issuers to be innovative with the credit card products offered?',\n",
       "  'answer': 'Intense competition, market saturation, and changing consumer postures have forced issuers to be innovative with the credit card products offered.'},\n",
       " {'question': 'How has risk-based pricing allowed banks to issue cards to less-qualified applicants?',\n",
       "  'answer': 'Risk-based pricing has allowed banks to issue cards to less-qualified applicants in exchange for a higher interest rate or other fees and to essentially offer customized card products.'}]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get answer from user and give correctness score out of 100\n",
    "\n",
    "question_1 = response[0]['question']\n",
    "answer_1 = response[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find correctness we cant rely on cosine similarity. but need to build a chain/agent that evaluate the answer\n",
    "user_answer_1 = \"Fierce competition and market saturation are key factors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_answer_1_embedding = OpenAIEmbeddings().embed_query(user_answer_1)\n",
    "answer_1_embedding = OpenAIEmbeddings().embed_query(answer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8662348409772439\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "\n",
    "from langchain.evaluation import EmbeddingDistance\n",
    "evaluator2 = load_evaluator(\n",
    "    \"pairwise_embedding_distance\", distance_metric=EmbeddingDistance.COSINE\n",
    ")\n",
    "\n",
    "score_1 = evaluator2.evaluate_string_pairs(\n",
    "    prediction=user_answer_1, prediction_b= answer_1\n",
    ")\n",
    "print(1-score_1['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reasoning': 'Both Assistant A and Assistant B provided the same response to the user\\'s question. They both correctly identified \"fierce competition\" and \"market saturation\" as factors that have forced credit card issuers to be innovative with their products. However, neither of them mentioned the third factor, \"changing consumer postures\", which was included in the reference answer. Therefore, both responses lack depth and completeness. Given these considerations, my evaluation results in a tie. \\n\\nFinal Verdict: [[C]]',\n",
       " 'value': None,\n",
       " 'score': 0.5}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "evaluator = load_evaluator(\"labeled_pairwise_string\")\n",
    "\n",
    "evaluator.evaluate_string_pairs(\n",
    "    prediction=user_answer_1,\n",
    "    prediction_b = user_answer_1,\n",
    "    input=question_1,\n",
    "    reference=answer_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
